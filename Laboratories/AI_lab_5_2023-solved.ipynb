{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8210b19",
   "metadata": {},
   "source": [
    "## A.I. Assignment 5\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this lab, you should be able to:\n",
    "* Get more familiar with tensors in pytorch \n",
    "* Create a simple multilayer perceptron model with pytorch\n",
    "* Visualise the parameters\n",
    "\n",
    "\n",
    "### Task\n",
    "\n",
    "Build a fully connected feed forward network that adds two bits. Determine the a propper achitecture for this network (what database you use for this problem? how many layers? how many neurons on each layer? what is the activation function? what is the loss function? etc)\n",
    "\n",
    "Create at least 3 such networks and compare their performance (how accurate they are?, how farst they are trained to get at 1 accuracy?)\n",
    "\n",
    "Display for the best one the weights for each layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3614e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5ee7e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# For this problem of adding two bits, we can treat it as a binary classification problem where the input consists of two bits\n",
    "# (binary digits) and the output is the sum of these two bits. We'll create three fully connected feedforward neural networks \n",
    "# and compare their performance.\n",
    "\n",
    "# Hidden Layer:The hidden layer(s) of a neural network are intermediary layers between the input and output layers.\n",
    "# Each neuron in the hidden layer receives input from the previous layer, applies a transformation (determined by its weights and\n",
    "# activation function), and passes the result to the next layer.\n",
    "# The hidden layer is responsible for learning and extracting features from the input data, transforming it into a form that is useful for making predictions.\n",
    "# In the provided architecture, there is one hidden layer with 32 neurons. This means there are 32 units or nodes in this hidden layer, each performing its own weighted sum of inputs and applying an activation function to produce an output.\n",
    "# Output Layer:The output layer is the final layer of the neural network, responsible for producing the network's output.\n",
    "\n",
    "# Each neuron in the output layer typically represents a different class or prediction value.\n",
    "# The number of neurons in the output layer depends on the nature of the problem. For example, in a binary classification problem like the one described (adding two bits), there are two possible outputs (0 or 1), so the output layer has 2 neurons.\n",
    "# The output layer applies its own transformation to the information it receives from the previous layer(s), often using a different activation function than the hidden layers.\n",
    "# In the provided architecture, the output layer has 2 neurons, each producing an output corresponding to one of the possible binary sums of the two input bits.\n",
    "# In summary, the main difference between the hidden layer and the output layer lies in their roles within the neural network. The hidden layer processes and transforms the input data to extract useful features, while the output layer produces the final prediction or classification based on the processed information from the hidden layers.\n",
    "model1 = nn.Sequential(OrderedDict([\n",
    "    ('hidden_net', nn.Linear(2,32)), #2(input layer)-(32-32)hidden-2(output layer)-search pic of neural network\n",
    "    ('hidden_act', nn.Sigmoid()),  \n",
    "    ('output_net', nn.Linear(32,2)),\n",
    "    ('output_act', nn.Sigmoid())  \n",
    "]))\n",
    "model2 = nn.Sequential(OrderedDict([\n",
    "    ('hidden_net', nn.Linear(2,8)),\n",
    "    ('hidden_act', nn.ReLU()),\n",
    "    ('output_net', nn.Linear(8,2)),\n",
    "    ('output_act', nn.Sigmoid())\n",
    "]))\n",
    "model3 = nn.Sequential(OrderedDict([\n",
    "    ('hidden_net', nn.Linear(2,16)),\n",
    "    ('hidden_act', nn.Sigmoid()),\n",
    "    ('output_net', nn.Linear(16,2)),\n",
    "    ('output_act', nn.Sigmoid())\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "665ae958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (hidden_net): Linear(in_features=2, out_features=32, bias=True)\n",
      "  (hidden_act): Sigmoid()\n",
      "  (output_net): Linear(in_features=32, out_features=2, bias=True)\n",
      "  (output_act): Sigmoid()\n",
      ")\n",
      "Sequential(\n",
      "  (hidden_net): Linear(in_features=2, out_features=8, bias=True)\n",
      "  (hidden_act): ReLU()\n",
      "  (output_net): Linear(in_features=8, out_features=2, bias=True)\n",
      "  (output_act): Sigmoid()\n",
      ")\n",
      "Sequential(\n",
      "  (hidden_net): Linear(in_features=2, out_features=16, bias=True)\n",
      "  (hidden_act): Sigmoid()\n",
      "  (output_net): Linear(in_features=16, out_features=2, bias=True)\n",
      "  (output_act): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model1)\n",
    "print(model2)\n",
    "print(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e26f0d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "data_in = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float) #add them, we get 0, 1, 1, 2\n",
    "print(data_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fb16bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "data_target = torch.tensor([[0, 0], [0, 1], [0, 1], [1, 0]], dtype=torch.float) #this is 0,1,1,2 in base 2 (1 0)=2=1+1 in base 10\n",
    "print(data_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69d920ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "criterion1 = nn.MSELoss() #mean squared error loss function\n",
    "# MSE loss measures the mean squared difference between the predicted values and the target values. It's commonly used for \n",
    "#regression problems.\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.01)\n",
    "#Adam is an optimization algorithm that adapts the learning rate for each parameter individually.\n",
    "#Here, model1.parameters() provides the parameters (weights and biases) of the neural network model to be optimized, and\n",
    "#lr=0.01 sets the learning rate for the optimizer to 0.01.-In traditional optimization algorithms like stochastic gradient \n",
    "# descent (SGD), a single learning rate is applied to update all parameters of the model. However, in many cases, different\n",
    "# parameters might require different learning rates for effective training. Some parameters may need larger updates, while \n",
    "# others may need smaller updates. it starts from lr=0.01 and it can change it on each param\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "optimizer2 = torch.optim.SGD(model2.parameters(), lr=0.01, momentum=0.9)\n",
    "#stochastic gradient descent\n",
    "criterion3 = nn.L1Loss()\n",
    "optimizer3 = torch.optim.SGD(model3.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cde91f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# Train the model\n",
    "def train(model, inputs, outputs, criterion, optimizer):\n",
    "    for epoch in range(10000):\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(inputs), outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        outputs = model(data_in)\n",
    "        predicted = (outputs >=0.5).float()\n",
    "        accuracy = (predicted == data_target).float().mean()\n",
    "        if accuracy == 1:\n",
    "            print(f'Training Accuracy: {accuracy.item()*100} in {epoch+1} epochs, loss: {loss}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dff3ec1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.]])\n",
      "Training Accuracy: 62.5\n",
      "tensor([[0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.]])\n",
      "Training Accuracy: 62.5\n",
      "tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]])\n",
      "Training Accuracy: 37.5\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "for model in [model1, model2, model3]:\n",
    "    if model == model1:\n",
    "        criterion = criterion1\n",
    "        optimizer = optimizer1\n",
    "    if model == model2:\n",
    "        criterion = criterion2\n",
    "        optimizer = optimizer2\n",
    "    if model == model3:\n",
    "        criterion = criterion3\n",
    "        optimizer = optimizer3\n",
    "    train(model, data_in, data_target, criterion, optimizer)\n",
    "    outputs = model(data_in)\n",
    "    predicted = (outputs >=0.5).float()\n",
    "    print(predicted)\n",
    "    accuracy = (predicted == data_target).float().mean()\n",
    "    print(f'Training Accuracy: {accuracy.item()*100}')\n",
    "# model(data_in): This syntax calls the model as if it were a function, passing the input data data_in to it. In PyTorch, this\n",
    "# invokes the forward method of the model, which performs the forward pass computation through the neural network layers.\n",
    "# The input data data_in propagates through the network, and the model generates predictions (output activations) for each input\n",
    "# sample.\n",
    "\n",
    "# outputs: This variable stores the output predictions generated by the neural network model for the input data data_in. The \n",
    "# outputs tensor typically contains real-valued numbers representing probabilities or scores for different classes or \n",
    "# categories. In the provided context, it likely contains the model's predictions for the binary sums of the input pairs\n",
    "# of bits.\n",
    "# predicted == data_target-This comparison operation compares each element of the predicted tensor (which contains the binary predictions generated by \n",
    "# the model) with the corresponding element of the data_target tensor (which contains the ground truth or true labels).\n",
    "# This operation results in a new boolean tensor where each element is True if the prediction matches the target label and False\n",
    "# otherwise.\n",
    "\n",
    "# .float(): This method converts the boolean tensor obtained from the previous step into a float tensor. This conversion is\n",
    "# necessary to perform arithmetic operations with these values.\n",
    "\n",
    "# .mean(): This method calculates the mean (average) of all the elements in the tensor. Since the boolean values (True and False)\n",
    "# are treated as 1.0 and 0.0 respectively after the conversion to float, taking the mean effectively computes the accuracy of the\n",
    "# predictions.\n",
    "\n",
    "# Specifically, the mean operation calculates the fraction of correct predictions out of the total number of predictions made. \n",
    "# If the predictions match the targets, the corresponding elements in the boolean tensor are 1.0, contributing to the numerator\n",
    "# of the mean calculation. Otherwise, they are 0.0, contributing to the denominator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1a7518b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_net.weight tensor([[-3.8360, -4.1430],\n",
      "        [-3.7472, -3.7722],\n",
      "        [-4.8880, -4.9714],\n",
      "        [-1.9762, -2.6197],\n",
      "        [-1.4855,  3.0303],\n",
      "        [ 3.8673,  3.6824],\n",
      "        [-2.4565, -2.5139],\n",
      "        [ 3.7762,  3.5859],\n",
      "        [-3.9446, -3.7539],\n",
      "        [ 2.5841,  2.1582],\n",
      "        [ 3.8317, -4.7734],\n",
      "        [-1.0257, -0.7952],\n",
      "        [-4.8148, -4.8131],\n",
      "        [-4.7856,  4.0958],\n",
      "        [-4.5492,  5.5350],\n",
      "        [-3.4252, -3.2786],\n",
      "        [-3.8055, -4.0219],\n",
      "        [-3.3417, -3.3775],\n",
      "        [ 4.2742,  4.3602],\n",
      "        [-2.6796, -1.1075],\n",
      "        [-3.7367, -3.6491],\n",
      "        [-3.5608, -3.4065],\n",
      "        [ 4.2890,  4.1696],\n",
      "        [-2.1985, -2.6443],\n",
      "        [-1.9615, -2.3075],\n",
      "        [ 2.6682,  2.9772],\n",
      "        [-4.2441, -4.2478],\n",
      "        [-3.4053, -3.2821],\n",
      "        [-3.7131, -3.5721],\n",
      "        [-2.6542, -2.7966],\n",
      "        [-3.2136, -3.1327],\n",
      "        [ 5.5110, -4.7183]])\n",
      "hidden_net.bias tensor([ 1.6048,  1.4834,  2.0299,  3.0857,  0.8781, -5.5808,  1.3373, -5.4368,\n",
      "         5.6978, -2.1839, -2.1167, -0.0398,  1.9789, -2.2451,  2.4040,  4.9271,\n",
      "         1.5726,  4.9028, -1.8463,  2.0332,  1.4431,  5.1356, -6.2715,  3.2662,\n",
      "         2.8802, -1.5739,  1.7349,  4.9254,  5.3823,  3.8935,  4.6454,  2.4919])\n",
      "output_net.weight tensor([[-1.2460, -1.1149, -1.0335, -1.7112,  0.9285,  2.7360, -1.2815,  3.0096,\n",
      "         -2.0265,  1.8443, -1.6189, -0.5332, -1.1523, -1.6877,  1.7872, -2.0285,\n",
      "         -1.1758, -2.2812,  0.9815, -1.7581, -1.2320, -1.7194,  2.4613, -1.8763,\n",
      "         -1.7064,  1.6644, -0.8519, -2.0202, -2.0502, -1.6689, -1.9963,  1.7352],\n",
      "        [-3.6644, -3.3583, -4.2776,  0.6383, -1.5339, -3.5435, -1.4660, -3.7635,\n",
      "          2.6114,  0.5326,  3.5423, -0.4484, -4.5275,  3.9658, -4.0665,  2.2556,\n",
      "         -3.7224,  2.0044,  2.2558,  0.0925, -3.5236,  2.2118, -3.2054,  0.6044,\n",
      "          0.9119,  1.4211, -3.3782,  2.6992,  2.6516,  1.1295,  2.0637, -4.0735]])\n",
      "output_net.bias tensor([ 0.3697, -1.0454])\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "for name, param in model1.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdf09ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bea66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29c65a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
